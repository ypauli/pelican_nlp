"""
Perplexity extraction module for the Pelican NLP pipeline.

This module provides functionality to calculate perplexity metrics from logits data
generated by the logits extraction step.
"""

import os
import pandas as pd
import numpy as np
import io
from pathlib import Path
from typing import List, Tuple, Dict, Any

from pelican_nlp.config import debug_print
from pelican_nlp.utils.csv_functions import store_features_to_csv


class PerplexityExtractor:
    """Extract perplexity metrics from logits CSV files."""
    
    def __init__(self, options: Dict[str, Any], project_folder: Path):
        """
        Initialize the PerplexityExtractor.
        
        Args:
            options: Configuration options for perplexity extraction
            project_folder: Path to the project folder
        """
        self.options = options
        self.project_folder = project_folder
        self.derivatives_dir = project_folder / 'derivatives'
        
    def extract_perplexity_from_document(self, document, logits_data: List[Dict[str, Any]]) -> None:
        """
        Extract perplexity metrics from logits data for a single document.
        
        Args:
            document: Document object containing metadata
            logits_data: List of logits dictionaries from logits extraction
        """
        debug_print(f"Extracting perplexity for document: {document.name}")
        
        # Convert logits data to DataFrame format expected by perplexity calculation
        if not logits_data:
            debug_print(f"No logits data found for document: {document.name}")
            return
            
        # Create DataFrame from logits data
        df = pd.DataFrame(logits_data)
        
        # Calculate perplexity metrics
        section_perplexities, sentence_perplexities_per_section = self._calculate_perplexity_metrics(df)
        
        # Store results
        self._store_perplexity_results(document, section_perplexities, sentence_perplexities_per_section)
        
    def _calculate_perplexity_metrics(self, df: pd.DataFrame) -> Tuple[List[float], List[List[float]]]:
        """
        Calculate perplexity metrics from logits DataFrame.
        
        Args:
            df: DataFrame with logits data (token, logprob_actual, logprob_max, entropy, most_likely_token)
            
        Returns:
            Tuple of (section_perplexities, sentence_perplexities_per_section)
        """
        # Split into sections based on the header pattern
        sections = self._split_into_sections(df)
        
        section_perplexities = []
        sentence_perplexities_per_section = []
        
        for section_df in sections:
            if section_df.empty:
                section_perplexities.append(None)
                sentence_perplexities_per_section.append([])
                continue
                
            # Extract prompt and main text
            prompt_df, main_df = self._extract_prompt(section_df)
            
            # Calculate section-level perplexity (excluding prompt)
            if "logprob_actual" in main_df.columns and not main_df.empty:
                n_main = len(main_df)
                logprob_sum_main = main_df["logprob_actual"].sum()
                section_perplexity = np.exp(-logprob_sum_main / n_main)
            else:
                section_perplexity = None
                
            section_perplexities.append(section_perplexity)
            
            # Calculate sentence-level perplexities
            sentence_perplexities = []
            
            # Prompt is always sentence 0
            if not prompt_df.empty and "logprob_actual" in prompt_df.columns:
                n_prompt = len(prompt_df)
                logprob_sum_prompt = prompt_df["logprob_actual"].sum()
                prompt_perplexity = np.exp(-logprob_sum_prompt / n_prompt)
            else:
                prompt_perplexity = None
                
            sentence_perplexities.append(prompt_perplexity)
            
            # Split main text into sentences
            sentences = self._split_df_into_sentences(main_df)
            
            for sent_df in sentences:
                if sent_df.empty or "logprob_actual" not in sent_df.columns:
                    sentence_perplexities.append(None)
                else:
                    n_sent = len(sent_df)
                    logprob_sum = sent_df["logprob_actual"].sum()
                    sent_perplexity = np.exp(-logprob_sum / n_sent)
                    sentence_perplexities.append(sent_perplexity)
                    
            sentence_perplexities_per_section.append(sentence_perplexities)
            
        return section_perplexities, sentence_perplexities_per_section
    
    def _split_into_sections(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """
        Split DataFrame into sections based on header pattern.
        This mimics the behavior of the original perplexity.py file.
        """
        # Look for the header pattern that indicates a new section
        header_line = "token,logprob_actual,logprob_max,entropy,most_likely_token"
        
        # Convert DataFrame to string representation to find section boundaries
        sections = []
        current_section = []
        
        # Convert DataFrame to list of rows for processing
        df_rows = df.to_dict('records')
        
        for row in df_rows:
            # Check if this row represents a header (all values should match the header pattern)
            if (row.get('token') == 'token' and 
                row.get('logprob_actual') == 'logprob_actual' and
                row.get('logprob_max') == 'logprob_max' and
                row.get('entropy') == 'entropy' and
                row.get('most_likely_token') == 'most_likely_token'):
                
                # If we have accumulated data, create a section
                if current_section:
                    section_df = pd.DataFrame(current_section)
                    sections.append(section_df)
                current_section = [row]  # Start new section with header
            else:
                current_section.append(row)
        
        # Add the last section
        if current_section:
            section_df = pd.DataFrame(current_section)
            sections.append(section_df)
            
        # If no sections were found, treat the entire DataFrame as one section
        if not sections:
            sections = [df]
            
        return sections
    
    def _extract_prompt(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Extract prompt from the beginning of the DataFrame.
        
        Args:
            df: DataFrame with token data
            
        Returns:
            Tuple of (prompt_df, remainder_df)
        """
        if df.empty:
            return pd.DataFrame(), df
            
        # Get ignore sequences from config
        ignore_sequences = self.options.get('ignore_sequences', [])
        
        for sequence in ignore_sequences:
            seq_len = len(sequence)
            if len(df) >= seq_len:
                top_tokens = list(df["token"].iloc[:seq_len])
                top_tokens = [str(token).strip('"') for token in top_tokens]
                if top_tokens == sequence:
                    prompt_part = df.iloc[:seq_len].copy()
                    remainder_part = df.iloc[seq_len:].copy().reset_index(drop=True)
                    return prompt_part, remainder_part
                    
        return pd.DataFrame(), df
    
    def _split_df_into_sentences(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """
        Split DataFrame into sentences based on period tokens.
        
        Args:
            df: DataFrame with token data
            
        Returns:
            List of DataFrames, one per sentence
        """
        sentences = []
        current_rows = []
        
        for idx in df.index:
            row = df.loc[idx]
            current_rows.append(row)
            if "." in str(row["token"]):
                sentence_df = pd.DataFrame(current_rows).reset_index(drop=True)
                sentences.append(sentence_df)
                current_rows = []
                
        if current_rows:
            sentence_df = pd.DataFrame(current_rows).reset_index(drop=True)
            sentences.append(sentence_df)
            
        return sentences
    
    def _store_perplexity_results(self, document, section_perplexities: List[float], 
                                 sentence_perplexities_per_section: List[List[float]]) -> None:
        """
        Store perplexity results to CSV files.
        
        Args:
            document: Document object
            section_perplexities: List of section-level perplexity values
            sentence_perplexities_per_section: List of sentence-level perplexity lists per section
        """
        # Store section-level perplexity
        if self.options.get('calculate_section_perplexity', True):
            section_data = []
            for i, perplexity in enumerate(section_perplexities):
                section_data.append({
                    'section_index': i,
                    'perplexity': perplexity
                })
            
            if section_data:
                store_features_to_csv(section_data, self.derivatives_dir, document, metric='perplexity-section')
        
        # Store sentence-level perplexity
        if self.options.get('calculate_sentence_perplexity', True):
            sentence_data = []
            for section_idx, sentence_perplexities in enumerate(sentence_perplexities_per_section):
                for sent_idx, perplexity in enumerate(sentence_perplexities):
                    sentence_data.append({
                        'section_index': section_idx,
                        'sentence_index': sent_idx,
                        'perplexity': perplexity
                    })
            
            if sentence_data:
                store_features_to_csv(sentence_data, self.derivatives_dir, document, metric='perplexity-sentence')
