"""
This module provides functionality to calculate perplexity metrics from logits data
generated by the logits extraction step.
"""

import os
import pandas as pd
import numpy as np
import io
import string
from pathlib import Path
from typing import List, Tuple, Dict, Any

from pelican_nlp.config import debug_print
from pelican_nlp.utils.csv_functions import store_features_to_csv


class PerplexityExtractor:
    """Extract perplexity metrics from logits CSV files."""
    
    def __init__(self, options: Dict[str, Any], project_folder: Path):
        """
        Initialize the PerplexityExtractor.
        
        Args:
            options: Configuration options for perplexity extraction
            project_folder: Path to the project folder
        """
        self.options = options
        self.project_folder = project_folder
        self.derivatives_dir = project_folder / 'derivatives'
        
    def extract_perplexity_from_document(self, document, logits_data: List[Dict[str, Any]], section_index: int = 0) -> None:
        """
        Extract perplexity metrics from logits data for a single section of a document.
        
        Note: Each logits_data corresponds to one section that was already identified
        at the document level, so we don't need to detect sections again.
        
        Args:
            document: Document object containing metadata
            logits_data: List of logits dictionaries from logits extraction (one section)
            section_index: Index of the section being processed (for proper indexing in results)
        """
        debug_print(f"Extracting perplexity for document: {document.name}, section: {section_index}")
        
        # Convert logits data to DataFrame format expected by perplexity calculation
        if not logits_data:
            debug_print(f"No logits data found for document: {document.name}, section: {section_index}")
            return
            
        # Create DataFrame from logits data
        df = pd.DataFrame(logits_data)
        
        # Calculate perplexity metrics (treating entire DataFrame as one section)
        section_perplexities, sentence_perplexities_per_section = self._calculate_perplexity_metrics(df)
        
        # Store results with correct section index
        self._store_perplexity_results(document, section_perplexities, sentence_perplexities_per_section, section_index)
        
    def _calculate_perplexity_metrics(self, df: pd.DataFrame) -> Tuple[List[float], List[List[float]]]:
        """
        Calculate perplexity metrics from logits DataFrame.
        
        Note: Each logits_data already corresponds to one section from the document,
        so we treat the entire DataFrame as a single section (no need to split).
        
        Args:
            df: DataFrame with logits data (token, logprob_actual, logprob_max, entropy, most_likely_token)
            
        Returns:
            Tuple of (section_perplexities, sentence_perplexities_per_section)
            where section_perplexities is a list with one element (this section)
            and sentence_perplexities_per_section is a list with one list (sentences for this section)
        """
        section_perplexities = []
        sentence_perplexities_per_section = []
        
        # Treat the entire DataFrame as a single section (sections are already separated at document level)
        if df.empty:
            section_perplexities.append(None)
            sentence_perplexities_per_section.append([])
            return section_perplexities, sentence_perplexities_per_section
                
        # Extract prompt and main text
        prompt_df, main_df = self._extract_prompt(df)
        
        # Calculate section-level perplexity (excluding prompt)
        if "logprob_actual" in main_df.columns and not main_df.empty:
            n_main = len(main_df)
            logprob_sum_main = main_df["logprob_actual"].sum()
            section_perplexity = np.exp(-logprob_sum_main / n_main)
        else:
            section_perplexity = None
            
        section_perplexities.append(section_perplexity)
        
        # Calculate sentence-level perplexities
        sentence_perplexities = []
        
        # Prompt is always sentence 0
        if not prompt_df.empty and "logprob_actual" in prompt_df.columns:
            n_prompt = len(prompt_df)
            logprob_sum_prompt = prompt_df["logprob_actual"].sum()
            prompt_perplexity = np.exp(-logprob_sum_prompt / n_prompt)
        else:
            prompt_perplexity = None
            
        sentence_perplexities.append(prompt_perplexity)
        
        # Split main text into sentences
        sentences = self._split_df_into_sentences(main_df)
        
        for sent_df in sentences:
            if sent_df.empty or "logprob_actual" not in sent_df.columns:
                sentence_perplexities.append(None)
            else:
                # Count non-punctuation tokens (excluding punctuation-only tokens)
                non_punct_count = self._count_non_punctuation_tokens(sent_df)
                # Filter out single-word sentences (perplexity is meaningless for a single word)
                if non_punct_count <= 1:
                    # Skip single-word sentences - don't calculate perplexity
                    continue
                n_sent = len(sent_df)
                logprob_sum = sent_df["logprob_actual"].sum()
                sent_perplexity = np.exp(-logprob_sum / n_sent)
                sentence_perplexities.append(sent_perplexity)
                
        sentence_perplexities_per_section.append(sentence_perplexities)
            
        return section_perplexities, sentence_perplexities_per_section
    
    def _extract_prompt(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Extract prompt from the beginning of the DataFrame.
        
        Args:
            df: DataFrame with token data
            
        Returns:
            Tuple of (prompt_df, remainder_df)
        """
        if df.empty:
            return pd.DataFrame(), df
            
        # Get ignore sequences from config
        ignore_sequences = self.options.get('ignore_sequences', [])
        
        for sequence in ignore_sequences:
            seq_len = len(sequence)
            if len(df) >= seq_len:
                top_tokens = list(df["token"].iloc[:seq_len])
                top_tokens = [str(token).strip('"') for token in top_tokens]
                if top_tokens == sequence:
                    prompt_part = df.iloc[:seq_len].copy()
                    remainder_part = df.iloc[seq_len:].copy().reset_index(drop=True)
                    return prompt_part, remainder_part
                    
        return pd.DataFrame(), df
    
    def _split_df_into_sentences(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """
        Split DataFrame into sentences based on period tokens.
        
        Args:
            df: DataFrame with token data
            
        Returns:
            List of DataFrames, one per sentence
        """
        sentences = []
        current_rows = []
        
        for idx in df.index:
            row = df.loc[idx]
            current_rows.append(row)
            if "." in str(row["token"]):
                sentence_df = pd.DataFrame(current_rows).reset_index(drop=True)
                sentences.append(sentence_df)
                current_rows = []
                
        if current_rows:
            sentence_df = pd.DataFrame(current_rows).reset_index(drop=True)
            sentences.append(sentence_df)
            
        return sentences
    
    def _is_punctuation_token(self, token: str) -> bool:
        """
        Check if a token is punctuation-only.
        
        Args:
            token: Token string to check
            
        Returns:
            True if token is punctuation-only, False otherwise
        """
        token_str = str(token)
        # Handle special tokens
        SPECIAL_TOKENS = {'<s>', '</s>', '<pad>', '<unk>'}
        if token_str in SPECIAL_TOKENS:
            return True
        
        # Remove sentencepiece prefix if present
        token_core = token_str.replace('â–', '').strip()
        if token_core == '':
            return True
        
        # Check if all characters are punctuation
        return all(char in string.punctuation for char in token_core)
    
    def _count_non_punctuation_tokens(self, df: pd.DataFrame) -> int:
        """
        Count the number of non-punctuation tokens in a DataFrame.
        
        Args:
            df: DataFrame with token data
            
        Returns:
            Number of non-punctuation tokens
        """
        if df.empty or "token" not in df.columns:
            return 0
        
        count = 0
        for token in df["token"]:
            if not self._is_punctuation_token(token):
                count += 1
        return count
    
    def _store_perplexity_results(self, document, section_perplexities: List[float], 
                                 sentence_perplexities_per_section: List[List[float]], 
                                 section_index_offset: int = 0) -> None:
        """
        Store perplexity results to CSV files.
        
        Args:
            document: Document object
            section_perplexities: List of section-level perplexity values (should have one element)
            sentence_perplexities_per_section: List of sentence-level perplexity lists per section (should have one list)
            section_index_offset: Offset to add to section_index (for tracking across multiple sections)
        """
        # Store section-level perplexity
        if self.options.get('calculate_section_perplexity', True):
            section_data = []
            for i, perplexity in enumerate(section_perplexities):
                section_data.append({
                    'section_index': section_index_offset + i,
                    'perplexity': perplexity
                })
            
            if section_data:
                store_features_to_csv(section_data, self.derivatives_dir, document, metric='perplexity-section')
        
        # Store sentence-level perplexity
        if self.options.get('calculate_sentence_perplexity', True):
            sentence_data = []
            for section_idx, sentence_perplexities in enumerate(sentence_perplexities_per_section):
                actual_section_idx = section_index_offset + section_idx
                for sent_idx, perplexity in enumerate(sentence_perplexities):
                    sentence_data.append({
                        'section_index': actual_section_idx,
                        'sentence_index': sent_idx,
                        'perplexity': perplexity
                    })
            
            if sentence_data:
                store_features_to_csv(sentence_data, self.derivatives_dir, document, metric='perplexity-sentence')
